# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10uxHRwtVYzu8Jk-VXtH4xfuhaieDzldo
"""

import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
#import matplotlib.pyplot as plt
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

#upload train
from google.colab import files
uploaded=files.upload()

#upload test
from google.colab import files
uploaded_1=files.upload()

train_df=pd.read_csv('sign_mnist_train.csv')
test_df=pd.read_csv('sign_mnist_test.csv')

#preproccessing
y_train = train_df['label'].values
y_test = test_df['label'].values
X_train = train_df.drop('label', axis=1).values
X_test = test_df.drop('label', axis=1).values

X_train = X_train.reshape(-1, 28, 28)
X_test = X_test.reshape(-1, 28, 28)

print(f"Training data shape: {X_train.shape}")
print(f"Training labels shape: {y_train.shape}")

class SignLanguageDataset(Dataset):
    def __init__(self, images, labels, transform=None):
        self.images = images
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        image = self.images[idx]
        label = self.labels[idx]
        image = image.astype(np.float32) / 255.0
        image = np.expand_dims(image, axis=0)
        image_tensor = torch.tensor(image, dtype=torch.float32)
        label_tensor = torch.tensor(label, dtype=torch.long)

        return image_tensor, label_tensor

train_dataset = SignLanguageDataset(X_train, y_train)
test_dataset = SignLanguageDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

class SignLanguageCNN(nn.Module):
    def __init__(self):
        super(SignLanguageCNN, self).__init__()

        # Update to 25 classes as labels range from 0 to 24 (excluding 9)
        self.num_classes = 25


        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)


        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)


        self.dropout = nn.Dropout(0.5)


        self.fc1 = nn.Linear(32 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, self.num_classes)

    def forward(self, x):

        x = self.pool1(F.relu(self.conv1(x)))


        x = self.pool2(F.relu(self.conv2(x)))


        x = x.view(-1, 32 * 7 * 7)


        x = self.dropout(x)


        x = F.relu(self.fc1(x))


        x = self.fc2(x)
        return x

model = SignLanguageCNN() # Re-initialize the model
EPOCHS = 10
LEARNING_RATE = 0.001

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE) # Re-initialize the optimizer

for epoch in range(EPOCHS):
    model.train() # Set model to training mode
    running_loss = 0.0

    for images, labels in train_loader:


        # 1. Zero the gradients
        optimizer.zero_grad()

        # 2. Forward pass
        outputs = model(images)

        # 3. Calculate loss
        loss = criterion(outputs, labels)

        # 4. Backward pass
        loss.backward()

        # 5. Update weights
        optimizer.step()

        running_loss += loss.item()

    epoch_loss = running_loss / len(train_loader)
    print(f"Epoch {epoch+1}/{EPOCHS}, Training Loss: {epoch_loss:.4f}")

print("Training finished!")

# Evaluate the model on the test set
model.eval() # Set the model to evaluation mode
correct = 0
total = 0

with torch.no_grad(): # Disable gradient calculation during evaluation
    for images, labels in test_loader:
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = 100 * correct / total
print(f'Accuracy of the model on the test images: {accuracy:.2f}%')

from sklearn.metrics import classification_report, accuracy_score
import numpy as np

model.eval() # Set model to evaluation mode (disables dropout)

all_labels = []
all_predictions = []

with torch.no_grad(): # No need to calculate gradients during evaluation
    for images, labels in test_loader:
        # Move data to the appropriate device (CPU or GPU) if needed
        # images, labels = images.to(device), labels.to(device) # Assuming 'device' is defined

        # Forward pass
        outputs = model(images)

        # Get predictions
        # torch.max returns (values, indices)
        _, predicted = torch.max(outputs.data, 1)

        # Store labels and predictions
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# --- Calculate Metrics ---

# 1. Overall Accuracy (to check the >= 90% goal)
accuracy = accuracy_score(all_labels, all_predictions)
print(f"\nOverall Accuracy: {accuracy * 100:.2f}%")

# 2. Per-Class Accuracy (and other metrics)
# Get unique labels from the test set and create class names based on those unique labels
unique_labels = np.unique(all_labels)
class_names = [mapped_labels[i] for i in sorted(unique_labels)]

report = classification_report(all_labels, all_predictions, target_names=class_names)
print("\n--- Classification Report ---")
print(report)

# --- Add this line to the END of your training script ---
torch.save(model.state_dict(), 'sign_language_cnn.pth')
print("Model saved to sign_language_cnn.pth")

import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import time # Import time for potential frame rate control
from google.colab.patches import cv2_imshow # Import cv2_imshow

# --- 1. DEFINE THE CNN MODEL (Must be same as training) ---
# Copy-paste the SignLanguageCNN class definition from your training script.
class SignLanguageCNN(nn.Module):
    def __init__(self):
        super(SignLanguageCNN, self).__init__()
        self.num_classes = 25  # A-Y, excluding J and Z - Must match the trained model

        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.dropout = nn.Dropout(0.5)

        self.fc1 = nn.Linear(32 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, self.num_classes)

    def forward(self, x):
        x = self.pool1(F.relu(self.conv1(x)))
        x = self.pool2(F.relu(self.conv2(x)))
        x = x.view(-1, 32 * 7 * 7)
        x = self.dropout(x)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# --- 2. LOAD THE TRAINED MODEL ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = SignLanguageCNN().to(device)
# Assuming your model file is named 'sign_language_cnn.pth'
model.load_state_dict(torch.load('sign_language_cnn.pth', map_location=device))
model.eval()  # Set model to evaluation mode (disables dropout)

print("Model loaded successfully!")

# --- 3. CREATE LABEL MAPPING ---
# This maps the model's output (0-23) to a letter
alphabet = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
# J (9) and Z (25) are not in the dataset
mapped_labels = {i: char for i, char in enumerate(alphabet) if char != 'J' and char != 'Z'}

# --- 4. OPENCV VIDEO PROCESSING ---

# Define the Region of Interest (ROI) box (adjust as needed for your video)
ROI_SIZE = 400  # <--- You can make this 300, 400, 450 etc.
ROI_COLOR = (0, 255, 0)  # Green


# Replace with the path to your uploaded video file
video_path = '/content/hand.mp4'
cap = cv2.VideoCapture(video_path)

if not cap.isOpened():
    print(f"Error: Could not open video file: {video_path}")
    exit()

# Get frame dimensions after opening the video
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

# Calculate center
center_x = frame_width // 2
center_y = frame_height // 2

# Calculate top-left (X1, Y1) and bottom-right (X2, Y2)
ROI_X1 = center_x - (ROI_SIZE // 2)
ROI_Y1 = center_y - (ROI_SIZE // 2)
ROI_X2 = center_x + (ROI_SIZE // 2)
ROI_Y2 = center_y + (ROI_SIZE // 2)

# Ensure the box doesn't go off-screen
ROI_X1 = max(0, ROI_X1)
ROI_Y1 = max(0, ROI_Y1)
ROI_X2 = min(frame_width - 1, ROI_X2)
ROI_Y2 = min(frame_height - 1, ROI_Y2)

# Get video properties for display control (optional)
# fps = cap.get(cv2.CAP_PROP_FPS)
# if fps > 0:
#     frame_delay = 1 / fps
# else:
#     frame_delay = 0.03 # Default to ~30 fps if cannot get video fps

while True:
    # Read a frame from the video
    ret, frame = cap.read()
    if not ret:
        print("End of video stream.")
        break # Break the loop if no frame is read

    # Flip the frame horizontally (like a mirror) - Might not be needed for pre-recorded video
    # frame = cv2.flip(frame, 1)

    # Draw the ROI box on the frame
    cv2.rectangle(frame, (ROI_X1, ROI_Y1), (ROI_X2, ROI_Y2), ROI_COLOR, 2)

    # --- 5. PREDICTION ---

    # 1. Extract the ROI
    roi = frame[ROI_Y1:ROI_Y2, ROI_X1:ROI_X2]

    # 2. Convert to Grayscale
    gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)

    # 3. Resize to 28x28 (the size our model expects)
    resized_roi = cv2.resize(gray_roi, (28, 28), interpolation=cv2.INTER_AREA)

    # 4. Invert colors (MNIST is white-on-black, a hand is light-on-dark)
    # We use a simple threshold to make it look more like the dataset
    # This is a *critical* step and may need tuning
    _, preprocessed_roi = cv2.threshold(resized_roi, 120, 255, cv2.THRESH_BINARY)
    # (Comment out the line above and uncomment the line below if your
    # background is lighter than your hand)
    # _, preprocessed_roi = cv2.threshold(resized_roi, 120, 255, cv2.THRESH_BINARY_INV)


    # 5. Format for PyTorch
    # Add channel dim (1, 28, 28) and batch dim (1, 1, 28, 28)
    # Normalize to 0.0 - 1.0
    image_tensor = torch.tensor(preprocessed_roi, dtype=torch.float32) / 255.0
    image_tensor = image_tensor.unsqueeze(0).unsqueeze(0).to(device)

    # 6. Make a prediction
    with torch.no_grad():  # We don't need to calculate gradients
        output = model(image_tensor)
        _, predicted = torch.max(output.data, 1)

        predicted_class = predicted.item()
        prediction_text = mapped_labels.get(predicted_class, "Unknown")

    # --- 6. DISPLAY THE RESULT ---

    # Put the prediction text above the ROI box
    cv2.putText(frame, prediction_text, (ROI_X1, ROI_Y1 - 10),
                cv2.FONT_HERSHEY_SIMPLEX, 0.9, ROI_COLOR, 2)

    # Display the preprocessed 28x28 image (to see what the model sees)
    # We need to resize it so we can see it
    debug_img = cv2.resize(preprocessed_roi, (200, 200), interpolation=cv2.INTER_NEAREST)
    cv2_imshow(debug_img) # Use cv2_imshow

    # Display the main camera feed
    cv2_imshow(frame) # Use cv2_imshow

    # Add a small delay to control playback speed (optional)
    # time.sleep(frame_delay)


    # Check for 'q' key press to exit
    # cv2.waitKey(1) & 0xFF == ord('q') is not supported with cv2_imshow
    # You would need a different mechanism to stop the loop in Colab,
    # like a button or a fixed number of frames.
    # For now, the loop will run until the video ends.


# --- 7. CLEAN UP ---
cap.release()
# cv2.destroyAllWindows() is not needed with cv2_imshow